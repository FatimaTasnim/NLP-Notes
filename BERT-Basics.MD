# BIDIRECTIONAL ENCODER REPRESENTATION OF TRANSFORMER
BERT is a Language Representation Model. BERT is designed to pretrain deep bidirectional representations from unlabled text by jointly conditioning on both left and right context in all layers. It presents State-of-the-art results in a wide variety of NLP tasks.

BERT intruduce Bidirectional training of tranformer that learns contextual relations between words (or sub-words) in a text as a BIDIRECTIONALLU TRAINED MODEL can make deeper sense than unidirectional Models. BERT uses a technique (MLM) which allows bidirectional training.

## THE WAY BART WORKS
Instead of reading sequentially BERT reads the entire sequence at once. So, in that sense BERT actually knows the data from both direction. 

### BERT uses two training strategies
- Masked LM (MLM)
- Next Sentence Prediction (NSP)


